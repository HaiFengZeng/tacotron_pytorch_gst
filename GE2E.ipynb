{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm1 = nn.LSTM(input_size=20,hidden_size=768,batch_first=False)\n",
    "# lstm2 = nn.LSTM(input_size=256,hidden_size=768,batch_first=False)\n",
    "# lstm3 = nn.LSTM(input_size=256,hidden_size=768,batch_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out1,(hidden1,cell1) = lstm1(x)\n",
    "# out2,(hidden2,cell2) = lstm2(out1)\n",
    "# out3,(hidden3,cell3) = lstm3(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell1.size()#[num layers* direction,batch,hidden]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out1.size()#[T,B,H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeakerEncoder(nn.Module):\n",
    "    def __init__(self,input_size,N,M,hidden_size=768,project_size=256):\n",
    "        super(SpeakerEncoder,self).__init__()\n",
    "        self.w = nn.Parameter(torch.tensor(10.0))\n",
    "        self.b = nn.Parameter(torch.tensor(-5.0))\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size,hidden_size=hidden_size,batch_first=False)\n",
    "        self.project1 = nn.Linear(hidden_size,project_size)\n",
    "        self.lstm2 = nn.LSTM(input_size=project_size,hidden_size=hidden_size,batch_first=False)\n",
    "        self.project2 = nn.Linear(hidden_size,project_size)\n",
    "        self.lstm3 = nn.LSTM(input_size=project_size,hidden_size=hidden_size,batch_first=False)\n",
    "        self.project3 = nn.Linear(hidden_size,project_size)\n",
    "        \n",
    "    def similarity_matrix(self,x):\n",
    "        N,M = self.N,self.M\n",
    "        # x [N*M,d] B=N*M,d is a vector\n",
    "        c = x.split([M]*N,0)\n",
    "        c = torch.mean(torch.stack(c,0),1)# centroids [N,d]\n",
    "        y = x.unsqueeze(1).repeat(1,N,1)  #[N,N*M,d]\n",
    "        c1 = c.unsqueeze(0).repeat(N*M,1,1) #[N,N*M,d]\n",
    "        similarity = self.w*F.cosine_similarity(y,c1,dim=-1)+ self.b\n",
    "        return similarity \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x,(h0,c0) = self.lstm1(x)\n",
    "        x = x.permute(1,0,2)\n",
    "        x = self.project1(x)\n",
    "        x = x.permute(1,0,2)\n",
    "        x,(h0,c0) = self.lstm2(x)\n",
    "        x = x.permute(1,0,2)\n",
    "        x = self.project2(x)\n",
    "        x = x.permute(1,0,2)\n",
    "        x,(h0,c0) = self.lstm3(x)\n",
    "        x = x.permute(1,0,2)\n",
    "        x = self.project3(x)\n",
    "        x = x.permute(1,0,2)\n",
    "        x = x[-1,:,:]\n",
    "        # l2 norm\n",
    "        x = x/torch.norm(x)\n",
    "        return self.similarity_matrix(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_encoder = SpeakerEncoder(40,5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(100,50,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = speaker_encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 5])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GE2ELoss(nn.Module):\n",
    "    def __init__(self,N,M,loss_type='softmax'):\n",
    "        super(GE2ELoss,self).__init__()\n",
    "        self.N=N\n",
    "        self.M=M\n",
    "        assert loss_type in ['softmax','contrast']\n",
    "        self.loss_type = loss_type\n",
    "    def softmax(self,x):#论文里的这个地方说是最优化loss,应该是-loss\n",
    "        N,M = self.N,self.M\n",
    "        # x [N*M,N] ==> [N,M,N]\n",
    "        c = x.split([M]*N,0)\n",
    "        c = torch.stack(c,0)# centroids [N,M,N]\n",
    "        c = F.softmax(c,-1)\n",
    "        return -torch.sum(torch.sum(c,1)*torch.eye(N))\n",
    "    def contrast(self,x):\n",
    "        N,M = self.N,self.M\n",
    "        c = x.split([M]*N,0)\n",
    "        c = torch.stack(c,0)# centroids [N,M,N]\n",
    "        y = F.sigmoid(x)-F.sigmoid(x.max(-1)[0].unsqueeze(2).repeat(1,1,N))\n",
    "        return -torch.sum(torch.sum(y,1)*torch.eye(N))\n",
    "    def forward(self,similarity_matrix):\n",
    "        if self.loss_type =='softmax':\n",
    "            return self.softmax(similarity_matrix) \n",
    "        else:\n",
    "            return self.contrast(similarity_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_f = GE2ELoss(5,10)\n",
    "loss = loss_f(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_type = 'TD-SV'# 'TI-SV'\n",
    "hidden_size,project_size = 128,64\n",
    "if mode_type == 'TI-SV':\n",
    "    hidden_size,project_size = 768,256\n",
    "model = SpeakerEncoder(input_size=40,N=5,M=10,hidden_size=hidden_size,project_size=project_size)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "scheduler = StepLR(optimizer,step_size=30*1e6,gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.gather(t,1,torch.tensor([[1,1,1],[1,1,1],[0,1,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
